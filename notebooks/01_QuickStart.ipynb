{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EdcellenceTQM Quick Start Guide\n",
    "\n",
    "This notebook provides a quick introduction to the EdcellenceTQM framework, demonstrating the six core assessment equations.\n",
    "\n",
    "## Contents\n",
    "1. Basic ADLI Process Scoring (Equation 1)\n",
    "2. Basic LeTCI Results Scoring (Equation 2)\n",
    "3. Category Aggregation (Equation 3)\n",
    "4. Organizational Score (Equation 4)\n",
    "5. Integration Health Index (Equation 5)\n",
    "6. Gap-Based Prioritization (Equation 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from adli_letci_core import (\n",
    "    ADLIIndicators,\n",
    "    LeTCIIndicators,\n",
    "    compute_adli_score,\n",
    "    compute_letci_score,\n",
    "    compute_category_score,\n",
    "    compute_organizational_score,\n",
    "    compute_integration_health_index,\n",
    "    compute_gap_priority_score,\n",
    "    classify_maturity_level,\n",
    "    AssessmentEngine\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"EdcellenceTQM framework loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ADLI Process Scoring (Equation 1)\n",
    "\n",
    "Compute process item scores from four dimensional indicators:\n",
    "- **Approach** (P_A): Appropriateness and effectiveness of methods\n",
    "- **Deployment** (P_D): Extent of implementation\n",
    "- **Learning** (P_L): Refinement through evaluation\n",
    "- **Integration** (P_I): Organizational alignment\n",
    "\n",
    "**Equation**: $S^P_i = 100 \\times (w_A \\cdot P_A + w_D \\cdot P_D + w_L \\cdot P_L + w_I \\cdot P_I)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Score a Leadership process item\n",
    "leadership_item = ADLIIndicators(\n",
    "    approach=0.80,      # 80% effectiveness of leadership methods\n",
    "    deployment=0.75,    # 75% deployment across organization\n",
    "    learning=0.70,      # 70% refinement through evaluation\n",
    "    integration=0.80    # 80% strategic alignment\n",
    ")\n",
    "\n",
    "score = compute_adli_score(leadership_item)\n",
    "\n",
    "print(f\"Process Item Score: {score:.2f}/100\")\n",
    "print(f\"\\nDimensional Breakdown:\")\n",
    "print(f\"  Approach:     {leadership_item.approach:.2f} × 30% = {leadership_item.approach * 0.30:.3f}\")\n",
    "print(f\"  Deployment:   {leadership_item.deployment:.2f} × 30% = {leadership_item.deployment * 0.30:.3f}\")\n",
    "print(f\"  Learning:     {leadership_item.learning:.2f} × 20% = {leadership_item.learning * 0.20:.3f}\")\n",
    "print(f\"  Integration:  {leadership_item.integration:.2f} × 20% = {leadership_item.integration * 0.20:.3f}\")\n",
    "print(f\"  Total (×100): {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LeTCI Results Scoring (Equation 2)\n",
    "\n",
    "Evaluate results items across four dimensions:\n",
    "- **Level** (R_Lv): Current performance level\n",
    "- **Trend** (R_Tr): Rate and direction of improvement\n",
    "- **Comparison** (R_Cp): Performance vs. benchmarks\n",
    "- **Integration** (R_I): Cross-category alignment\n",
    "\n",
    "**Equation**: $S^R_j = 100 \\times (w_{Lv} \\cdot R_{Lv} + w_{Tr} \\cdot R_{Tr} + w_{Cp} \\cdot R_{Cp} + w_I \\cdot R_I)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Score a Results item (Student Learning Outcomes)\n",
    "results_item = LeTCIIndicators(\n",
    "    level=0.85,         # 85% current performance level\n",
    "    trend=0.80,         # 80% positive trend strength\n",
    "    comparison=0.75,    # 75% vs. benchmark universities\n",
    "    integration=0.85    # 85% alignment with strategic goals\n",
    ")\n",
    "\n",
    "results_score = compute_letci_score(results_item)\n",
    "\n",
    "print(f\"Results Item Score: {results_score:.2f}/100\")\n",
    "print(f\"\\nDimensional Breakdown:\")\n",
    "print(f\"  Level:        {results_item.level:.2f} × 40% = {results_item.level * 0.40:.3f}\")\n",
    "print(f\"  Trend:        {results_item.trend:.2f} × 25% = {results_item.trend * 0.25:.3f}\")\n",
    "print(f\"  Comparison:   {results_item.comparison:.2f} × 25% = {results_item.comparison * 0.25:.3f}\")\n",
    "print(f\"  Integration:  {results_item.integration:.2f} × 10% = {results_item.integration * 0.10:.3f}\")\n",
    "print(f\"  Total (×100): {results_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Category Score Aggregation (Equation 3)\n",
    "\n",
    "Aggregate item scores into category scores using point-value weighting:\n",
    "\n",
    "**Equation**: $C_k = \\frac{\\sum (v_i \\cdot S_i)}{\\sum v_i}$\n",
    "\n",
    "where $v_i$ is the Baldrige point allocation for item $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Aggregate Leadership category (2 items)\n",
    "# Item 1.1: Senior Leadership (70 points)\n",
    "item_1_1 = ADLIIndicators(0.80, 0.75, 0.70, 0.80)\n",
    "score_1_1 = compute_adli_score(item_1_1)\n",
    "\n",
    "# Item 1.2: Governance and Societal Contributions (50 points)\n",
    "item_1_2 = ADLIIndicators(0.75, 0.70, 0.65, 0.75)\n",
    "score_1_2 = compute_adli_score(item_1_2)\n",
    "\n",
    "# Aggregate category score\n",
    "leadership_category_score = compute_category_score(\n",
    "    item_scores=[score_1_1, score_1_2],\n",
    "    item_point_values=[70, 50]\n",
    ")\n",
    "\n",
    "print(f\"Leadership Category Aggregation:\")\n",
    "print(f\"  Item 1.1 (70 pts): {score_1_1:.2f}\")\n",
    "print(f\"  Item 1.2 (50 pts): {score_1_2:.2f}\")\n",
    "print(f\"\\nWeighted Mean: ({score_1_1:.2f}×70 + {score_1_2:.2f}×50) / (70+50)\")\n",
    "print(f\"Category Score: {leadership_category_score:.2f}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Organizational Score (Equation 4)\n",
    "\n",
    "Compute organizational score as weighted sum of seven categories:\n",
    "\n",
    "**Equation**: $O = \\sum_{k=1}^{7} (W_k \\cdot C_k)$\n",
    "\n",
    "Categories: Leadership, Strategy, Customers, Measurement, Workforce, Operations, Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compute organizational score\n",
    "category_scores = {\n",
    "    'Leadership': 76.25,\n",
    "    'Strategy': 72.50,\n",
    "    'Customers': 74.00,\n",
    "    'Measurement': 82.00,\n",
    "    'Workforce': 68.50,\n",
    "    'Operations': 78.00,\n",
    "    'Results': 81.25\n",
    "}\n",
    "\n",
    "org_score = compute_organizational_score(category_scores)\n",
    "\n",
    "print(\"Organizational Score Computation:\")\n",
    "print(\"\\nCategory Scores (weighted):\")\n",
    "weights = {\n",
    "    'Leadership': 0.12,\n",
    "    'Strategy': 0.085,\n",
    "    'Customers': 0.085,\n",
    "    'Measurement': 0.10,\n",
    "    'Workforce': 0.10,\n",
    "    'Operations': 0.15,\n",
    "    'Results': 0.36\n",
    "}\n",
    "\n",
    "for category, score in category_scores.items():\n",
    "    contribution = score * weights[category]\n",
    "    print(f\"  {category:15} {score:5.2f} × {weights[category]:5.3f} = {contribution:6.2f}\")\n",
    "\n",
    "print(f\"\\nOrganizational Score: {org_score:.2f}/100\")\n",
    "\n",
    "# Classify maturity level\n",
    "maturity = classify_maturity_level(org_score)\n",
    "print(f\"Maturity Level: {maturity['level']} - {maturity['label']}\")\n",
    "print(f\"Description: {maturity['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integration Health Index (Equation 5)\n",
    "\n",
    "Measure cross-category integration strength:\n",
    "\n",
    "**Equation**: $IHI = \\frac{1}{2} \\left[ \\frac{1}{N_p}\\sum P_I + \\frac{1}{N_r}\\sum R_I \\right]$\n",
    "\n",
    "**Interpretation**:\n",
    "- IHI > 0.75: Strong integration\n",
    "- IHI 0.60-0.75: Moderate integration\n",
    "- IHI < 0.60: Weak integration (siloed operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compute IHI from integration dimensions\n",
    "# Process integration scores (from ADLI P_I)\n",
    "process_integration = [0.80, 0.75, 0.70, 0.72, 0.68, 0.78, 0.76]\n",
    "\n",
    "# Results integration scores (from LeTCI R_I)\n",
    "results_integration = [0.85, 0.82, 0.80, 0.78]\n",
    "\n",
    "ihi = compute_integration_health_index(\n",
    "    process_integration_scores=process_integration,\n",
    "    results_integration_scores=results_integration\n",
    ")\n",
    "\n",
    "print(\"Integration Health Index (IHI) Computation:\")\n",
    "print(f\"\\nProcess Integration (n={len(process_integration)}):\")\n",
    "print(f\"  Values: {process_integration}\")\n",
    "print(f\"  Mean: {np.mean(process_integration):.3f}\")\n",
    "\n",
    "print(f\"\\nResults Integration (n={len(results_integration)}):\")\n",
    "print(f\"  Values: {results_integration}\")\n",
    "print(f\"  Mean: {np.mean(results_integration):.3f}\")\n",
    "\n",
    "print(f\"\\nIHI = (0.5) × ({np.mean(process_integration):.3f} + {np.mean(results_integration):.3f})\")\n",
    "print(f\"IHI = {ihi:.3f}\")\n",
    "\n",
    "# Interpret IHI\n",
    "if ihi > 0.75:\n",
    "    interpretation = \"Strong cross-category integration\"\n",
    "elif ihi >= 0.60:\n",
    "    interpretation = \"Moderate integration with improvement opportunities\"\n",
    "else:\n",
    "    interpretation = \"Weak integration, siloed operations detected\"\n",
    "\n",
    "print(f\"\\nInterpretation: {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gap-Based Prioritization (Equation 6)\n",
    "\n",
    "Prioritize improvement initiatives based on:\n",
    "- Performance gap (target - current)\n",
    "- Point value (strategic importance)\n",
    "- Deployment urgency (fraction of org lacking implementation)\n",
    "\n",
    "**Equation**: $G_i = (T_i - S_i) \\cdot v_i \\cdot \\delta_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Prioritize improvement initiatives\n",
    "items = [\n",
    "    {'id': '1.1', 'name': 'Senior Leadership', 'score': 76.25, 'points': 70, 'urgency': 0.25},\n",
    "    {'id': '2.1', 'name': 'Strategy Development', 'score': 72.50, 'points': 40, 'urgency': 0.30},\n",
    "    {'id': '5.1', 'name': 'Workforce Capability', 'score': 68.50, 'points': 40, 'urgency': 0.80},\n",
    "    {'id': '6.1', 'name': 'Work Processes', 'score': 78.00, 'points': 50, 'urgency': 0.22},\n",
    "]\n",
    "\n",
    "priorities = []\n",
    "for item in items:\n",
    "    gap_priority = compute_gap_priority_score(\n",
    "        current_score=item['score'],\n",
    "        target_score=100.0,\n",
    "        point_value=item['points'],\n",
    "        deployment_urgency=item['urgency']\n",
    "    )\n",
    "    priorities.append({'item': item, 'priority': gap_priority})\n",
    "\n",
    "# Sort by priority (descending)\n",
    "priorities.sort(key=lambda x: x['priority'], reverse=True)\n",
    "\n",
    "print(\"Gap-Based Priority Ranking:\")\n",
    "print(\"\\nRank | Item | Score | Gap | Points | Urgency | Priority\")\n",
    "print(\"-\" * 70)\n",
    "for rank, p in enumerate(priorities, 1):\n",
    "    item = p['item']\n",
    "    gap = 100 - item['score']\n",
    "    print(f\"{rank:4} | {item['id']:4} | {item['score']:5.2f} | {gap:4.2f} | {item['points']:6} | {item['urgency']:7.2f} | {p['priority']:8.1f}\")\n",
    "\n",
    "print(f\"\\nTop Priority: {priorities[0]['item']['name']} (Priority Score: {priorities[0]['priority']:.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Priority Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize priorities\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of priority scores\n",
    "items_sorted = [p['item']['id'] for p in priorities]\n",
    "scores_sorted = [p['priority'] for p in priorities]\n",
    "colors = ['#d62728' if i == 0 else '#1f77b4' for i in range(len(priorities))]\n",
    "\n",
    "ax1.barh(items_sorted, scores_sorted, color=colors)\n",
    "ax1.set_xlabel('Gap Priority Score', fontsize=12)\n",
    "ax1.set_ylabel('Assessment Item', fontsize=12)\n",
    "ax1.set_title('Improvement Priority Ranking', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Scatter plot: Gap vs Urgency (sized by points)\n",
    "gaps = [100 - p['item']['score'] for p in priorities]\n",
    "urgencies = [p['item']['urgency'] for p in priorities]\n",
    "point_values = [p['item']['points'] for p in priorities]\n",
    "\n",
    "scatter = ax2.scatter(gaps, urgencies, s=[pv*5 for pv in point_values], \n",
    "                     c=scores_sorted, cmap='RdYlGn_r', alpha=0.6, edgecolors='black')\n",
    "ax2.set_xlabel('Performance Gap', fontsize=12)\n",
    "ax2.set_ylabel('Deployment Urgency', fontsize=12)\n",
    "ax2.set_title('Gap-Urgency Matrix (bubble size = point value)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Annotate points\n",
    "for i, p in enumerate(priorities):\n",
    "    ax2.annotate(p['item']['id'], (gaps[i], urgencies[i]), \n",
    "                fontsize=10, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "plt.colorbar(scatter, ax=ax2, label='Priority Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the six core assessment equations:\n",
    "\n",
    "1. **ADLI Scoring**: Process item assessment with dimensional weighting\n",
    "2. **LeTCI Scoring**: Results measurement emphasizing current level\n",
    "3. **Category Aggregation**: Point-value weighted means\n",
    "4. **Organizational Score**: Seven-category weighted sum with maturity classification\n",
    "5. **Integration Health Index**: Cross-category alignment measurement\n",
    "6. **Gap Prioritization**: Multi-factor improvement planning\n",
    "\n",
    "### Next Steps\n",
    "- **Notebook 02**: ADLI Analysis with radar charts\n",
    "- **Notebook 03**: LeTCI Results with trend visualization\n",
    "- **Notebook 04**: Complete organizational assessment pipeline\n",
    "- **Notebook 05**: 3D priority matrices\n",
    "- **Notebook 06**: IHI trajectory analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
