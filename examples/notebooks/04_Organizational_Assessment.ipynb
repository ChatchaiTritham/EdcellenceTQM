{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Organizational Assessment Pipeline\n",
    "\n",
    "This notebook demonstrates the complete assessment workflow from item-level scoring to organizational maturity classification.\n",
    "\n",
    "## Pipeline Steps\n",
    "1. Load assessment data\n",
    "2. Compute item scores (ADLI + LeTCI)\n",
    "3. Aggregate category scores\n",
    "4. Calculate organizational score\n",
    "5. Classify maturity level\n",
    "6. Generate comprehensive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from adli_letci_core import (\n",
    "    AssessmentEngine,\n",
    "    ADLIIndicators,\n",
    "    LeTCIIndicators,\n",
    "    compute_category_score,\n",
    "    compute_organizational_score,\n",
    "    classify_maturity_level\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Assessment engine loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Assessment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "df = pd.read_csv('../data/examples/sample_assessment_data.csv')\n",
    "\n",
    "print(f\"Total assessments: {len(df)}\")\n",
    "print(f\"Departments: {df['department'].nunique()}\")\n",
    "print(f\"Process items: {len(df[df['item_type']=='Process'])}\")\n",
    "print(f\"Results items: {len(df[df['item_type']=='Results'])}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Assessment for Single Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_department(df, department_name):\n",
    "    \"\"\"\n",
    "    Run complete assessment pipeline for a department.\n",
    "    \"\"\"\n",
    "    dept_df = df[df['department'] == department_name]\n",
    "    \n",
    "    # Prepare process items\n",
    "    process_items = []\n",
    "    for _, row in dept_df[dept_df['item_type'] == 'Process'].iterrows():\n",
    "        process_items.append({\n",
    "            'item_id': row['item_id'],\n",
    "            'category': row['category'],\n",
    "            'adli': ADLIIndicators(\n",
    "                approach=row['approach'],\n",
    "                deployment=row['deployment'],\n",
    "                learning=row['learning'],\n",
    "                integration=row['integration']\n",
    "            ),\n",
    "            'point_value': row['point_value'],\n",
    "            'deployment_gap': 1.0 - row['deployment']  # Estimate urgency from deployment\n",
    "        })\n",
    "    \n",
    "    # Prepare results items\n",
    "    results_items = []\n",
    "    for _, row in dept_df[dept_df['item_type'] == 'Results'].iterrows():\n",
    "        results_items.append({\n",
    "            'item_id': row['item_id'],\n",
    "            'category': row['category'],\n",
    "            'letci': LeTCIIndicators(\n",
    "                level=row['level'],\n",
    "                trend=row['trend'],\n",
    "                comparison=row['comparison'],\n",
    "                integration=row['integration']\n",
    "            ),\n",
    "            'point_value': row['point_value'],\n",
    "            'deployment_gap': 1.0 - row['level']  # Estimate from level\n",
    "        })\n",
    "    \n",
    "    # Run assessment\n",
    "    engine = AssessmentEngine()\n",
    "    results = engine.compute_organizational_assessment(\n",
    "        process_items=process_items,\n",
    "        results_items=results_items,\n",
    "        category_point_allocations={}\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Assess Computer Science department\n",
    "cs_assessment = assess_department(df, 'Computer Science')\n",
    "\n",
    "print(\"Computer Science - Assessment Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nOrganizational Score: {cs_assessment['organizational_score']:.2f}/100\")\n",
    "print(f\"Integration Health Index: {cs_assessment['ihi']:.3f}\")\n",
    "print(f\"\\nMaturity Classification:\")\n",
    "print(f\"  Level: {cs_assessment['maturity_level']['level']}\")\n",
    "print(f\"  Label: {cs_assessment['maturity_level']['label']}\")\n",
    "print(f\"  Description: {cs_assessment['maturity_level']['description']}\")\n",
    "print(f\"\\nCategory Scores:\")\n",
    "for category, score in sorted(cs_assessment['category_scores'].items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {category:15}: {score:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Score Breakdown Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize category scores\n",
    "categories = list(cs_assessment['category_scores'].keys())\n",
    "scores = list(cs_assessment['category_scores'].values())\n",
    "\n",
    "# Sort by Baldrige order\n",
    "baldrige_order = ['Leadership', 'Strategy', 'Customers', 'Measurement', 'Workforce', 'Operations', 'Results']\n",
    "ordered_scores = [cs_assessment['category_scores'][cat] for cat in baldrige_order]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart\n",
    "colors_cat = ['#e74c3c' if s < 70 else '#f39c12' if s < 85 else '#27ae60' for s in ordered_scores]\n",
    "ax1.barh(baldrige_order, ordered_scores, color=colors_cat)\n",
    "ax1.set_xlabel('Category Score', fontsize=12)\n",
    "ax1.set_ylabel('Baldrige Category', fontsize=12)\n",
    "ax1.set_title('Computer Science - Category Scores', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlim(0, 100)\n",
    "ax1.axvline(70, color='orange', linestyle='--', alpha=0.5, label='Level 4 Threshold')\n",
    "ax1.axvline(85, color='green', linestyle='--', alpha=0.5, label='Level 5 Threshold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Pie chart with category weights\n",
    "weights = [0.12, 0.085, 0.085, 0.10, 0.10, 0.15, 0.36]\n",
    "contributions = [s * w for s, w in zip(ordered_scores, weights)]\n",
    "explode = [0.05 if cat == 'Results' else 0 for cat in baldrige_order]\n",
    "\n",
    "ax2.pie(contributions, labels=baldrige_order, autopct='%1.1f%%', \n",
    "       explode=explode, shadow=True, startangle=90)\n",
    "ax2.set_title('Contribution to Organizational Score\\n(Score Ã— Weight)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Department Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess all departments\n",
    "departments = df['department'].unique()\n",
    "all_assessments = {}\n",
    "\n",
    "for dept in departments:\n",
    "    all_assessments[dept] = assess_department(df, dept)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for dept, assessment in all_assessments.items():\n",
    "    comparison_data.append({\n",
    "        'Department': dept,\n",
    "        'Org Score': assessment['organizational_score'],\n",
    "        'IHI': assessment['ihi'],\n",
    "        'Maturity Level': assessment['maturity_level']['level'],\n",
    "        'Maturity Label': assessment['maturity_level']['label']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).sort_values('Org Score', ascending=False)\n",
    "\n",
    "print(\"\\nDepartment Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maturity Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Score distribution\n",
    "maturity_colors = {1: '#e74c3c', 2: '#e67e22', 3: '#f39c12', 4: '#27ae60', 5: '#2ecc71'}\n",
    "colors_list = [maturity_colors[level] for level in comparison_df['Maturity Level']]\n",
    "\n",
    "ax1.barh(comparison_df['Department'], comparison_df['Org Score'], color=colors_list)\n",
    "ax1.set_xlabel('Organizational Score', fontsize=12)\n",
    "ax1.set_ylabel('Department', fontsize=12)\n",
    "ax1.set_title('Organizational Scores by Department', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlim(0, 100)\n",
    "\n",
    "# Add maturity band shading\n",
    "ax1.axvspan(0, 20, alpha=0.1, color='red', label='Level 1')\n",
    "ax1.axvspan(21, 40, alpha=0.1, color='orange')\n",
    "ax1.axvspan(41, 60, alpha=0.1, color='yellow')\n",
    "ax1.axvspan(61, 85, alpha=0.1, color='lightgreen')\n",
    "ax1.axvspan(86, 100, alpha=0.1, color='green', label='Level 5')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# IHI vs Score scatter\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    ax2.scatter(row['IHI'], row['Org Score'], \n",
    "               s=200, alpha=0.6, \n",
    "               color=maturity_colors[row['Maturity Level']],\n",
    "               edgecolors='black', linewidth=2)\n",
    "    ax2.annotate(row['Department'], (row['IHI'], row['Org Score']),\n",
    "                fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "ax2.set_xlabel('Integration Health Index', fontsize=12)\n",
    "ax2.set_ylabel('Organizational Score', fontsize=12)\n",
    "ax2.set_title('IHI vs Organizational Performance', fontsize=14, fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_xlim(0, 1.0)\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Assessment Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_assessment_report(department, assessment):\n",
    "    \"\"\"\n",
    "    Generate formatted assessment report.\n",
    "    \"\"\"\n",
    "    report = f\"\"\"\n",
    "{'='*70}\n",
    "ORGANIZATIONAL EXCELLENCE ASSESSMENT REPORT\n",
    "{'='*70}\n",
    "\n",
    "Department: {department}\n",
    "Assessment Date: 2024-02-01\n",
    "\n",
    "OVERALL PERFORMANCE\n",
    "{'-'*70}\n",
    "Organizational Score:        {assessment['organizational_score']:6.2f}/100\n",
    "Integration Health Index:    {assessment['ihi']:6.3f}\n",
    "\n",
    "MATURITY CLASSIFICATION\n",
    "{'-'*70}\n",
    "Maturity Level:              {assessment['maturity_level']['level']} - {assessment['maturity_level']['label']}\n",
    "Description:                 {assessment['maturity_level']['description']}\n",
    "Score Range:                 {assessment['maturity_level']['range'][0]}-{assessment['maturity_level']['range'][1]}\n",
    "\n",
    "CATEGORY BREAKDOWN\n",
    "{'-'*70}\n",
    "\"\"\"\n",
    "    \n",
    "    for category in baldrige_order:\n",
    "        score = assessment['category_scores'][category]\n",
    "        report += f\"{category:15}: {score:5.2f}\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "TOP 5 IMPROVEMENT PRIORITIES\n",
    "{'-'*70}\n",
    "Rank | Item ID | Priority Score\n",
    "{'-'*70}\n",
    "\"\"\"\n",
    "    \n",
    "    for rank, (item_id, priority) in enumerate(assessment['gap_priorities'][:5], 1):\n",
    "        report += f\"{rank:4} | {item_id:7} | {priority:14.1f}\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "METADATA\n",
    "{'-'*70}\n",
    "Process Items Assessed:      {assessment['metadata']['process_items_count']}\n",
    "Results Items Assessed:      {assessment['metadata']['results_items_count']}\n",
    "Total Categories:            {assessment['metadata']['total_categories']}\n",
    "\n",
    "{'='*70}\n",
    "End of Report\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate report for Computer Science\n",
    "report = generate_assessment_report('Computer Science', cs_assessment)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all assessments to JSON format\n",
    "export_data = {\n",
    "    'assessment_date': '2024-02-01',\n",
    "    'institution': 'Rajamangala University of Technology Krungthep',\n",
    "    'departments': []\n",
    "}\n",
    "\n",
    "for dept, assessment in all_assessments.items():\n",
    "    dept_data = {\n",
    "        'department': dept,\n",
    "        'organizational_score': float(assessment['organizational_score']),\n",
    "        'ihi': float(assessment['ihi']),\n",
    "        'maturity_level': assessment['maturity_level']['level'],\n",
    "        'maturity_label': assessment['maturity_level']['label'],\n",
    "        'category_scores': {k: float(v) for k, v in assessment['category_scores'].items()},\n",
    "        'top_5_priorities': [\n",
    "            {'item_id': item_id, 'priority_score': float(score)}\n",
    "            for item_id, score in assessment['gap_priorities'][:5]\n",
    "        ]\n",
    "    }\n",
    "    export_data['departments'].append(dept_data)\n",
    "\n",
    "print(\"Assessment data exported to JSON format\")\n",
    "print(json.dumps(export_data, indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete organizational assessment pipeline:\n",
    "\n",
    "1. **Data Loading**: Import assessment data from CSV\n",
    "2. **Item Scoring**: Compute ADLI (process) and LeTCI (results) scores\n",
    "3. **Category Aggregation**: Point-value weighted means\n",
    "4. **Organizational Score**: Seven-category weighted sum\n",
    "5. **Maturity Classification**: Map scores to Baldrige maturity levels\n",
    "6. **Reporting**: Generate comprehensive assessment reports\n",
    "\n",
    "### Key Outputs:\n",
    "- Organizational score (0-100)\n",
    "- Integration Health Index (0-1)\n",
    "- Maturity level (1-5)\n",
    "- Category breakdown\n",
    "- Improvement priorities\n",
    "\n",
    "## Next Steps\n",
    "- **Notebook 05**: Gap prioritization with 3D matrices\n",
    "- **Notebook 06**: IHI trajectory analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
